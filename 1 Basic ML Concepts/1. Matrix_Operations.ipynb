{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831f083a",
   "metadata": {},
   "source": [
    "# Matrix operations\n",
    "In machine learning, matrix operations are fundamental for various tasks such as data manipulation, transformations, and computations. They enable efficient handling of large datasets and complex mathematical operations that are essential for training and deploying machine learning models. Understanding matrix operations is crucial for implementing algorithms like linear regression, neural networks, and more, as they often involve operations like matrix multiplication, transposition, and inversion. Mastery of these operations allows for optimized performance and scalability in machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbc7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837ec034",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.array([[1, 2, 1], [3, 4, 2], [4, 7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111abce3",
   "metadata": {},
   "source": [
    "## Matrix addition and subtraction\n",
    "Matrix addition is where two matrices of the same dimensions are added together by adding their corresponding elements.\n",
    "\n",
    "Matrix subtraction is where two matrices of the same dimensions are subtracted by subtracting their corresponding elements.\n",
    "\n",
    "In machine learning, matrix addition is often used in various algorithms and models, such as neural networks, where weights and biases are updated during training. It allows for efficient manipulation of data and parameters, enabling the combination of different features or datasets. Understanding matrix addition is crucial for implementing and optimizing machine learning algorithms, as it plays a key role in operations like gradient descent and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d400ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "Matrix B:\n",
      " [[1 2 1]\n",
      " [3 4 2]\n",
      " [4 7 8]]\n",
      "\n",
      "A + B:\n",
      " [[ 2  4  4]\n",
      " [ 7  9  8]\n",
      " [11 15 17]]\n",
      "\n",
      "A - B:\n",
      " [[0 0 2]\n",
      " [1 1 4]\n",
      " [3 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nMatrix B:\\n\", B)\n",
    "\n",
    "A_plus_B = A + B\n",
    "print(\"\\nA + B:\\n\", A_plus_B)\n",
    "\n",
    "A_minus_B = A - B\n",
    "print(\"\\nA - B:\\n\", A_minus_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b979d37",
   "metadata": {},
   "source": [
    "## Element-wise multiplication and division\n",
    "Element-wise multiplication (Hadamard product) is where two matrices of the same dimensions are multiplied together by multiplying their corresponding elements.\n",
    "\n",
    "Element-wise division is where two matrices of the same dimensions are divided by dividing their corresponding elements.\n",
    "\n",
    "In machine learning, element-wise multiplication and division are essential for various operations, such as applying activation functions, normalizing data, and performing feature scaling. These operations allow for efficient manipulation of data at the individual element level, enabling models to learn complex patterns and relationships within the data. Understanding element-wise operations is crucial for implementing and optimizing machine learning algorithms, as they often involve operations like dropout in neural networks and feature-wise transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d4e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A * B:\n",
      " [[ 1  4  3]\n",
      " [12 20 12]\n",
      " [28 56 72]]\n",
      "\n",
      "A / B:\n",
      " [[1.         1.         3.        ]\n",
      " [1.33333333 1.25       3.        ]\n",
      " [1.75       1.14285714 1.125     ]]\n"
     ]
    }
   ],
   "source": [
    "# Element wise multiplication of A and B\n",
    "A_times_B = np.multiply(A, B)\n",
    "print(\"\\nA * B:\\n\", A_times_B)\n",
    "\n",
    "A_div_B = np.divide(A, B)\n",
    "print(\"\\nA / B:\\n\", A_div_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64aeb6",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n",
    "Matrix multiplication (dot product) is where two matrices are multiplied together by taking the sum of the products of the corresponding elements from the rows of the first matrix and the columns of the second matrix.\n",
    "\n",
    "In machine learning, matrix multiplication is a fundamental operation used in various algorithms and models, such as linear regression, neural networks, and support vector machines. It enables the transformation of input data through layers of weights and biases, allowing models to learn complex relationships and patterns. Efficient matrix multiplication is crucial for handling large datasets and high-dimensional feature spaces, making it a key component in training and deploying machine learning models. Understanding matrix multiplication is essential for implementing and optimizing these algorithms effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d6cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A . B (Matrix Multiplication):\n",
      " [[ 19  31  29]\n",
      " [ 43  70  62]\n",
      " [ 67 109  95]]\n",
      "\n",
      "A . B with PyTorch:\n",
      " tensor([[ 19,  31,  29],\n",
      "        [ 43,  70,  62],\n",
      "        [ 67, 109,  95]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "A_dot_B = np.dot(A, B)\n",
    "print(\"\\nA . B (Matrix Multiplication):\\n\", A_dot_B)\n",
    "\n",
    "# With PyTorch (more about pytorch in next notebook)\n",
    "A_torch = torch.tensor(A)\n",
    "B_torch = torch.tensor(B)\n",
    "A_dot_B_torch = torch.matmul(A_torch, B_torch)\n",
    "# alternatively: A_dot_B_torch = A_torch @ B_torch\n",
    "print(\"\\nA . B with PyTorch:\\n\", A_dot_B_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e204f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
