{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9644b5fe",
   "metadata": {},
   "source": [
    "## Pre-Train a GPT from Scratch to be an ExpertGPT using PyTorch\n",
    "\n",
    "## By: Ishani Kathuria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9d16a",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and setup constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd998537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c719b",
   "metadata": {},
   "source": [
    "## Step 2: Load data\n",
    "\n",
    "I have scraped a bunch of recipes from allrecipes.com and saved them in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "457700cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data in characters: 2038449\n"
     ]
    }
   ],
   "source": [
    "input_file = r\"data\\allrecipes_data.txt\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "print(\"length of data in characters:\", sum(len(line) for line in text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba6751",
   "metadata": {},
   "source": [
    "## Step 3: Setup vocab and encode/decode functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0630aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vocab_size(text, tiktoken=False):\n",
    "\t\"\"\"Setup vocabulary size based on encoding method.\n",
    "\tArgs:\n",
    "\t\ttext (list of str): List of text lines.\n",
    "\t\ttiktoken (bool): Whether to use tiktoken for vocabulary.\n",
    "\tReturns:\n",
    "\t\tint: Vocabulary size.\n",
    "\t\"\"\"\n",
    "\tif tiktoken:\n",
    "\t\tvocab_size = ENCODER.n_vocab\n",
    "\n",
    "\t\tprint(\"vocab size based on tiktoken GPT-2 encoding:\", vocab_size)\n",
    "\telse:\n",
    "\t\tthe_chars = sorted(list(set(\" \".join(text))))\n",
    "\t\tvocab_size = len(the_chars)\n",
    "\n",
    "\t\tprint(\"vocab size based on unique characters:\", vocab_size)\n",
    "\t\tprint('chars:', ''.join(the_chars))\n",
    "\n",
    "\treturn vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, main_text=None, max_len=128, tiktoken=False):\n",
    "\t\"\"\"Encode text to a sequence of integers.\n",
    "\tArgs:\n",
    "\t\ttext (list of str): List of text lines to encode.\n",
    "\t\tmax_len (int): Maximum length of each encoded line.\n",
    "\t\ttiktoken (bool): Whether to use tiktoken for encoding.\n",
    "\tReturns:\n",
    "\t\ttorch.Tensor: Tensor of encoded integers.\n",
    "\t\"\"\"\n",
    "\tif tiktoken:\n",
    "\t\tall_tokens = []\n",
    "\t\tfor line in text:\n",
    "\t\t\ttokens = ENCODER.encode(line.strip())\n",
    "\t\t\t# truncate long recipes\n",
    "\t\t\ttokens = tokens[:max_len]\n",
    "\t\t\t# add separator between recipes\n",
    "\t\t\tall_tokens.extend(tokens + [ENCODER.eot_token])\n",
    "\telse:\n",
    "\t\tall_chars = sorted(list(set(\" \".join(main_text))))\n",
    "\t\tstoi = {ch: i for i, ch in enumerate(all_chars)}\n",
    "\n",
    "\t\tif text is None:\n",
    "\t\t\ttext = main_text\n",
    "\n",
    "\t\tall_tokens = []\n",
    "\t\tfor line in text:\n",
    "\t\t\ttokens = [stoi[ch] for ch in line.strip()]\n",
    "\t\t\t# truncate long recipes\n",
    "\t\t\ttokens = tokens[:max_len]\n",
    "\t\t\t# add separator between recipes\n",
    "\t\t\tall_tokens.extend(tokens + [stoi[' ']])\n",
    "\n",
    "\treturn torch.tensor(all_tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "def decode(tokens, text=None, tiktoken=False):\n",
    "\t\"\"\"Decode a sequence of integers back to text.\n",
    "\tArgs:\n",
    "\t\ttokens (list or torch.Tensor): Sequence of integers to decode.\n",
    "\t\ttext (str): Original text (required if not using tiktoken).\n",
    "\t\ttiktoken (bool): Whether to use tiktoken for decoding.\n",
    "\tReturns:\n",
    "\t\tstr: Decoded text.\n",
    "\t\"\"\"\n",
    "\tif tiktoken:\n",
    "\t\ttext = ENCODER.decode(tokens)\n",
    "\t\ttext = text.replace('<|endoftext|>', '\\n')\n",
    "\t\treturn text\n",
    "\telse:\n",
    "\t\tall_chars = sorted(list(set(\" \".join(text))))\n",
    "\t\titos = {i: ch for i, ch in enumerate(all_chars)}\n",
    "\t\ttext = ''.join([itos[token] for token in tokens])\n",
    "\t\treturn text.replace(' ', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf343bb",
   "metadata": {},
   "source": [
    "## Step 4: Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff7ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, train_ratio=0.9, train=True):\n",
    "    n = int(train_ratio * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train data has {len(train_data)} tokens\")\n",
    "        data_ = train_data\n",
    "    else:\n",
    "        print(f\"Validation data has {len(val_data)} tokens\")\n",
    "        data_ = val_data\n",
    "\n",
    "    max_start = len(data_) - BLOCK_SIZE\n",
    "    if max_start <= 0:\n",
    "        raise ValueError(\n",
    "            f\"Data too small for block_size={BLOCK_SIZE}. Reduce block_size.\")\n",
    "\n",
    "    ix = torch.randint(max_start, (BATCH_SIZE,))\n",
    "    x = torch.stack([data_[i: i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data_[i+1: i+1+BLOCK_SIZE] for i in ix])\n",
    "\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01742ea3",
   "metadata": {},
   "source": [
    "## Step 5: Create NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01cc1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBED, head_size, bias=False)\n",
    "\n",
    "        tril_def = torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))\n",
    "        self.register_buffer('tril', tril_def)\n",
    "        self.dropout = nn.Dropout(DROPOUT_VAL)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        head_size = k.size(-1)\n",
    "        wei = q @ k.transpose(-2, -1) * head_size ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ae3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT_VAL),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe67e996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(DROPOUT_VAL)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52f09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2727b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_emb_table = nn.Embedding(BLOCK_SIZE, n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head=n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_ffw_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=DEVICE))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_ffw_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, E = logits.shape\n",
    "            logits = logits.view(B*T, E)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b408bb",
   "metadata": {},
   "source": [
    "## Step 6: Setup loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "017a03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(train_data, val_data):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            if split == 'train':\n",
    "                X, Y = train_data\n",
    "            else:\n",
    "                X, Y = val_data\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3b8c9",
   "metadata": {},
   "source": [
    "## Step 7: Setup initial params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d46ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(256)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BLOCK_SIZE = 40  # N tokens in sequence\n",
    "BATCH_SIZE = 64\n",
    "MAX_ITERS = 6000\n",
    "EVAL_ITERS = 300\n",
    "EVAL_INTERVAL = 500\n",
    "\n",
    "LEARNING_RATE = 0.0003\n",
    "N_EMBED = 512\n",
    "N_HEAD = 8  # 8 attention heads\n",
    "N_LAYER = 6  # 6 encoder layers\n",
    "DROPOUT_VAL = 0.2\n",
    "\n",
    "# other constants\n",
    "ENCODER = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e626318",
   "metadata": {},
   "source": [
    "## Step 8: Train without tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fbd2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size based on unique characters: 110\n",
      "chars: \n",
      " !\"$%&'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz ®°¼½¾áèéñûĆćč​–—‘’“”⁄™⅓⅔⅛⅜\n",
      "\n",
      "Encoded data: tensor([48, 65, 76,  ..., 27,  1,  1])\n",
      "Encoded data shape: torch.Size([1559067]) and dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = setup_vocab_size(text, tiktoken=False)\n",
    "data = encode(text, main_text=text, tiktoken=False)\n",
    "\n",
    "print(\"\\nEncoded data:\", data)\n",
    "print(\"Encoded data shape: {} and dtype: {}\".format(data.shape, data.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c95f7f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 1403160 tokens\n",
      "\n",
      "Train data X shape: torch.Size([64, 40])\n",
      "Train data Y shape: torch.Size([64, 40])\n",
      "Validation data has 155907 tokens\n",
      "\n",
      "Validation data X shape: torch.Size([64, 40])\n",
      "Validation data Y shape: torch.Size([64, 40])\n"
     ]
    }
   ],
   "source": [
    "train_data = get_batch(data, train=True)\n",
    "print(\"\\nTrain data X shape:\", train_data[0].shape)\n",
    "print(\"Train data Y shape:\", train_data[1].shape)\n",
    "\n",
    "val_data = get_batch(data, train=False)\n",
    "print(\"\\nValidation data X shape:\", val_data[0].shape)\n",
    "print(\"Validation data Y shape:\", val_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b674c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tn_embed=N_EMBED,\n",
    "\tn_head=N_HEAD,\n",
    "\tn_layer=N_LAYER,\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13fac071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8732, val loss 4.8785\n",
      "step 500: train loss 0.0344, val loss 6.7613\n",
      "step 1000: train loss 0.0340, val loss 7.0988\n",
      "step 1500: train loss 0.0334, val loss 7.4959\n",
      "step 2000: train loss 0.0334, val loss 7.5826\n",
      "step 2500: train loss 0.0336, val loss 7.6140\n",
      "step 3000: train loss 0.0334, val loss 7.9874\n",
      "step 3500: train loss 0.0334, val loss 7.7455\n",
      "step 4000: train loss 0.0332, val loss 7.9353\n",
      "step 4500: train loss 0.0332, val loss 7.9278\n",
      "step 5000: train loss 0.0335, val loss 7.8881\n",
      "step 5500: train loss 0.0331, val loss 8.4531\n"
     ]
    }
   ],
   "source": [
    "# 25 mins\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss(train_data, val_data)\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = train_data\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5364b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"recipe_gpt_weights.pt\")\n",
    "torch.save(model, \"recipe_gpt_full.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcea158",
   "metadata": {},
   "source": [
    "## Step 9: Train with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "908c59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size based on tiktoken GPT-2 encoding: 50257\n",
      "\n",
      "Encoded data: tensor([19160,    25,  1879,  ...,  2559, 50256, 50256])\n",
      "Encoded data shape: torch.Size([554535]) and dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = setup_vocab_size(text, tiktoken=True)\n",
    "data = encode(text, tiktoken=True)\n",
    "\n",
    "print(\"\\nEncoded data:\", data)\n",
    "print(\"Encoded data shape: {} and dtype: {}\".format(data.shape, data.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b80ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data has 499081 tokens\n",
      "\n",
      "Train data X shape: torch.Size([64, 40])\n",
      "Train data Y shape: torch.Size([64, 40])\n",
      "Validation data has 55454 tokens\n",
      "\n",
      "Validation data X shape: torch.Size([64, 40])\n",
      "Validation data Y shape: torch.Size([64, 40])\n"
     ]
    }
   ],
   "source": [
    "train_data = get_batch(data, train=True)\n",
    "print(\"\\nTrain data X shape:\", train_data[0].shape)\n",
    "print(\"Train data Y shape:\", train_data[1].shape)\n",
    "\n",
    "val_data = get_batch(data, train=False)\n",
    "print(\"\\nValidation data X shape:\", val_data[0].shape)\n",
    "print(\"Validation data Y shape:\", val_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a18aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(\n",
    "\tvocab_size=VOCAB_SIZE,\n",
    "\tn_embed=N_EMBED,\n",
    "\tn_head=N_HEAD,\n",
    "\tn_layer=N_LAYER,\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62051b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.9975, val loss 10.9803\n",
      "step 500: train loss 0.0101, val loss 6.5823\n",
      "step 1000: train loss 0.0095, val loss 6.9171\n",
      "step 1500: train loss 0.0095, val loss 6.8513\n",
      "step 2000: train loss 0.0093, val loss 7.1076\n",
      "step 2500: train loss 0.0093, val loss 7.2503\n",
      "step 3000: train loss 0.0092, val loss 7.3446\n",
      "step 3500: train loss 0.0092, val loss 7.4494\n",
      "step 4000: train loss 0.0092, val loss 7.5193\n",
      "step 4500: train loss 0.0092, val loss 7.6105\n",
      "step 5000: train loss 0.0092, val loss 7.6700\n",
      "step 5500: train loss 0.0092, val loss 7.7287\n"
     ]
    }
   ],
   "source": [
    "# 29 mins\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss(train_data, val_data)\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = train_data\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7d7aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"recipe_gpt_weights_tiktoken.pt\")\n",
    "torch.save(model, \"recipe_gpt_full_tiktoken.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd68ab",
   "metadata": {},
   "source": [
    "## Evaluate saved models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f15dad",
   "metadata": {},
   "source": [
    "## Evaluate model without tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdc1231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size based on unique characters: 110\n",
      "chars: \n",
      " !\"$%&'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz ®°¼½¾áèéñûĆćč​–—‘’“”⁄™⅓⅔⅛⅜\n",
      "\n",
      "Encoded data: tensor([48, 65, 76,  ..., 27,  1,  1])\n",
      "Encoded data shape: torch.Size([1559067]) and dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = setup_vocab_size(text, tiktoken=False)\n",
    "data = encode(text, main_text=text, tiktoken=False)\n",
    "\n",
    "print(\"\\nEncoded data:\", data)\n",
    "print(\"Encoded data shape: {} and dtype: {}\".format(data.shape, data.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b74e64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding_table): Embedding(110, 512)\n",
       "  (pos_emb_table): Embedding(40, 512)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_ffw_head): Linear(in_features=512, out_features=110, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"recipe_gpt_full.pt\"\n",
    "model = torch.load(model_path, weights_only=False, map_location=DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "552d9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oncentrate\n",
      "-\n",
      "2tablespoonswater\n",
      "-\n",
      "3tables\n",
      "fr\n",
      "ver\n",
      "on\n",
      "on\n",
      "k\n",
      "-\n",
      "5\n",
      "S\n",
      "r\n",
      "1/25q\n",
      "Sm\n",
      "p\n",
      "onateat\n",
      "-\n",
      "1/4\n",
      "wixtum\n",
      "r\n",
      "-\n",
      "1/2con\n",
      "1/4\n",
      "slablablablaslalauchond\n",
      "onion.\n",
      "fr.\n",
      "ooonthopoooabover\n",
      "fr\n",
      "hehe\n",
      "f\n",
      "at\n",
      "fin\n",
      "ok\n",
      "f\n",
      "atu\n",
      "r\n",
      "d\n",
      "St\n",
      "St\n",
      "e\n",
      "-\n",
      "1\n",
      "fr\n",
      "-\n",
      "1/2tud\n",
      "1/4\n",
      "on\n",
      "3/4\n",
      "on\n",
      "Stlat\n",
      "1\n",
      "-\n",
      "1.\n",
      "Swile\n",
      "f\n",
      "on\n",
      "ov\n",
      "wily\n",
      "fr\n",
      "o\n",
      "°\n",
      "whe\n",
      "fr\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "ly\n",
      "Sablat\n",
      "f2m\n",
      "wixtu\n",
      "-\n",
      "1/4\n",
      "hon\n",
      "1/4\n",
      "1/4\n",
      "our\n",
      "-\n",
      "1/4\n",
      "S-\n",
      "1/4\n",
      "f-\n",
      "1/2cutablatlpoo\n",
      "1/4\n",
      "1/25y\n",
      "latlar\n",
      "oowlat.\n",
      "on.\n",
      "St.\n",
      "Pat\n",
      "oor\n",
      "owhe\n",
      "Sat\n",
      "St\n",
      "Sat\n",
      "over\n",
      "St\n",
      "at\n",
      "fry\n",
      "Stur\n",
      "salat\n",
      "f\n",
      "-\n",
      "1/4\n",
      "Stla/2.\n",
      "I================================\n"
     ]
    }
   ],
   "source": [
    "sos_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "generated_text = model.generate(sos_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, text=text, tiktoken=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b4525cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_32116\\3269294349.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Chicken recipe\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicken\n",
      "recipe\n",
      "sfred\n",
      "panfre\n",
      "ausil\n",
      "-\n",
      "(1/4\n",
      "f\n",
      "S\n",
      "f\n",
      "f\n",
      "-\n",
      "1/4\n",
      "f-\n",
      "1/25\n",
      "1/2chon\n",
      "1/4\n",
      "lilatoov.\n",
      "wiond\n",
      "-\n",
      "1-\n",
      "1\n",
      "on\n",
      "od\n",
      "fonud\n",
      "-\n",
      "1/4\n",
      "wilalabl\n",
      "-w\n",
      "-\n",
      "3/23/4\n",
      "-\n",
      "1/4\n",
      "fr\n",
      "1/4\n",
      "win\n",
      "1/4\n",
      "fry\n",
      "Stla\n",
      "-\n",
      "1/4\n",
      "fr\n",
      "Smmug\n",
      "-\n",
      "1/4\n",
      "on\n",
      "3/4\n",
      "or\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "latud\n",
      "-\n",
      "1/25\n",
      "-\n",
      "1/25\n",
      "fond\n",
      "-\n",
      "1/4\n",
      "on\n",
      "-\n",
      "1/4\n",
      "Blalablablat\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "1/4\n",
      "-\n",
      "1/2coond\n",
      "S-\n",
      "1/4\n",
      "1/4\n",
      "-\n",
      "1/2cuplatlatlatlachedddon\n",
      "-\n",
      "1/4\n",
      "on\n",
      "ond\n",
      "ond\n",
      "-\n",
      "S-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "25\n",
      "1/2latuchon\n",
      "-\n",
      "-laliondow-\n",
      "3/2latucon\n",
      "on\n",
      "fched\n",
      "on\n",
      "f-\n",
      "f-\n",
      "1/4\n",
      "175\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "25-\n",
      "1/4\n",
      "wablatblabond\n",
      "f-\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "on\n",
      "1/4\n",
      "2batpond\n",
      "S-\n",
      "jugres\n",
      "-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Chicken recipe\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, text=text, tiktoken=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68025443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_32116\\2111346345.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Viking Stew\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viking\n",
      "Stew\n",
      "sphonfreesth\n",
      "ch\n",
      "a\n",
      "2th\n",
      "ack\n",
      "pepat\n",
      "-\n",
      "onepfr\n",
      "-\n",
      "1/4\n",
      "and\n",
      "25\n",
      "on\n",
      "St\n",
      "-\n",
      "1/4\n",
      "w\n",
      "hed\n",
      "-\n",
      "1/2t\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "fr\n",
      "/4\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "on\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "on\n",
      "1/4\n",
      "feate\n",
      "-\n",
      "1/4\n",
      "f-\n",
      "1/2coon\n",
      "on\n",
      "-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "1/4\n",
      "on\n",
      "-\n",
      "1/23/4\n",
      "on\n",
      "-\n",
      "1/2cond\n",
      "on\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "1/4\n",
      "on\n",
      "-\n",
      "2/2cuplleon\n",
      "-\n",
      "1/4\n",
      "juorr\n",
      "f-\n",
      "1/4\n",
      "1/4\n",
      "on\n",
      "-\n",
      "1/4\n",
      "25\n",
      "r\n",
      "1/4\n",
      "on\n",
      "onilabl\n",
      "plalililat\n",
      "ove\n",
      "-\n",
      "1\n",
      "-\n",
      "1/25\n",
      "n\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "judd\n",
      "-\n",
      "1/4\n",
      "jujuilalilalil\n",
      "-\n",
      "1\n",
      "1/4\n",
      "fr\n",
      "1/4\n",
      "lilachoond\n",
      "pfr\n",
      "lat.\n",
      "oover\n",
      "n\n",
      "-\n",
      "1/25\n",
      "S\n",
      "S-\n",
      "1/4\n",
      "S\n",
      "S\n",
      "ablatopatlabr\n",
      "poon\n",
      "ond\n",
      "j-\n",
      "Sturry.\n",
      "wixtutummblatut\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Viking Stew\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, text=text, tiktoken=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb26a637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_11680\\2734706773.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Mushroom\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom\n",
      "t-\n",
      "1/2tablemponfreretablaconlesStu\n",
      "on\n",
      "-\n",
      "1\n",
      "oxt\n",
      "ddixtur\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "Studr\n",
      "Stuch-\n",
      "1/¼ephow\n",
      "1/4\n",
      "on\n",
      "on\n",
      "onextu\n",
      "elatu\n",
      "laton\n",
      "fr\n",
      "\n",
      "fr\n",
      "-\n",
      "1\n",
      "-\n",
      "1/23.\n",
      "fr\n",
      "fr\n",
      "-\n",
      "1/4\n",
      "Stheatut\n",
      "25\n",
      "f-\n",
      "1/4\n",
      "fr\n",
      "1/25\n",
      "-\n",
      "1/25\n",
      "y\n",
      "1/4\n",
      "fre\n",
      "-\n",
      "1.\n",
      "wilat\n",
      "ou\n",
      "on\n",
      "lilabl\n",
      "on\n",
      "oud\n",
      "-\n",
      "1\n",
      "he\n",
      "-\n",
      "1\n",
      "hichep\n",
      "on\n",
      "-\n",
      "1/4\n",
      "1/25-\n",
      "fr\n",
      "fr\n",
      "-\n",
      "ablacon\n",
      "wicon\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/2cheon\n",
      "ply\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/25ond\n",
      "-\n",
      "3\n",
      "one\n",
      "on\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "fro\n",
      "1/4\n",
      "-\n",
      "1/2conddlatudplathe\n",
      "-\n",
      "-\n",
      "fr\n",
      "1/2labled\n",
      "-\n",
      "-\n",
      "3/4\n",
      "on\n",
      "S\n",
      "lated\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "Stud\n",
      "-\n",
      "1/2cked\n",
      "-\n",
      "-\n",
      "3/4\n",
      "ooon\n",
      "-\n",
      "1/4\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "sababablab\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Mushroom\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, text=text, tiktoken=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e485098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_11680\\1049793302.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Salt\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt\n",
      "in\n",
      "the\n",
      "preheated\n",
      "oven\n",
      "for\n",
      "25\n",
      "minutes\n",
      "on\n",
      "f-\n",
      "1/4\n",
      "oveg\n",
      "f\n",
      "-\n",
      "1\n",
      "-\n",
      "1/23/4\n",
      "-\n",
      "1/23/2lablaondon\n",
      "f-\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "ond\n",
      "f-\n",
      "plablap-\n",
      "1/4\n",
      "fr\n",
      "filary\n",
      "1/4\n",
      "fr\n",
      "wil\n",
      "5\n",
      "-\n",
      "1/4\n",
      "wi-\n",
      "1/4\n",
      "1/4\n",
      "1/4\n",
      "1/4\n",
      "on\n",
      "Sm\n",
      "ow\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/2tatlatlat.\n",
      "jucher\n",
      "on\n",
      "1/4\n",
      "-\n",
      "1/4\n",
      "1/25-\n",
      "1/25\n",
      "Stud\n",
      "SBlaton\n",
      "f-\n",
      "3/4\n",
      "on\n",
      "1/4\n",
      "2chon\n",
      "onlatlat.\n",
      "oved\n",
      "S\n",
      "-\n",
      "1/4\n",
      "on\n",
      "25\n",
      "f-\n",
      "1/4\n",
      "fr\n",
      "Stus\n",
      "her\n",
      "fr\n",
      "atalacor\n",
      "1/4\n",
      "f-\n",
      "1/4\n",
      "fr\n",
      "1/4\n",
      "-\n",
      "1/2lablablabuhed\n",
      "-\n",
      "1/4\n",
      "-\n",
      "1/2½\n",
      "xtudpond\n",
      "-\n",
      "1.\n",
      "onat.\n",
      "fre\n",
      "fr\n",
      "Stu\n",
      "-\n",
      "1.\n",
      "Stud\n",
      "S\n",
      "at.\n",
      "he\n",
      "f\n",
      "S\n",
      "Sto\n",
      "S\n",
      "fr\n",
      "1/4\n",
      "fr\n",
      "\n",
      "d\n",
      "Stuila\n",
      "e\n",
      "f\n",
      "-\n",
      "1/25\n",
      "on\n",
      "1/4\n",
      "pouc\n",
      "on\n",
      "-.\n",
      "oory\n",
      "Stato\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Salt\"], main_text=text, tiktoken=False), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, text=text, tiktoken=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004db24",
   "metadata": {},
   "source": [
    "## Evaluate tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d91f27ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_embedding_table): Embedding(50257, 512)\n",
       "  (pos_emb_table): Embedding(40, 512)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_ffw_head): Linear(in_features=512, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_tiktoken = \"recipe_gpt_full_tiktoken.pt\"\n",
    "model_tiktoken = torch.load(model_path_tiktoken, weights_only=False, map_location=DEVICE)\n",
    "model_tiktoken.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e45d0fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!Dotdash Meredith Food Studios\n",
      "3. Using a 2 oz. (1/4-cup) ice-cream scoop, place 1 scoop of ice cream into the bottom of each bowl. Gather all ingredients.Dotdash Meredith Food Studios\n",
      "-shaped yogurt bringingsksemb vinegar, boneless chicken mixture, add chicken thighs\n",
      "-shaped354 with a fork, justshaped with a coatshaped yogurt oil in a bowl with a time, finely spoon for 9x13-shaped oil\n",
      "-shaped https://www.allrecipes.com/2.com/recipesk becomes with a funnel, cookedAllrecipes.com/recipe-skDotdash Meredith Food Studios\n",
      "-sk 1/2teaspo, finely chuck unlawful\n",
      "-sksk/2teaspo, finelyini covered oil\n",
      "- 1/2teaspo, finelyhaorape tomatoes in a 9x13- 1/2teaspo, bonelesscircleteaspo, finely Gather all sides Greek to taste\n",
      "-shaped oil\n",
      "-shaped oil\n",
      "- ¼cup garnazy- ¼cupbroccoli, finelypper- Has yogurt, add chicken broth to taste\n",
      "-shaped oil\n",
      "- ¼cuplow, finely 2cupshigh heat oil\n",
      "- ¼cupwater, boneless chicken broth\n",
      "- bamboo skewers, boneless chicken mixture, finelylow:\n",
      "-shaped oil\n",
      "-shaped oil\n",
      "- ¼cupItalian- ¼cupchoppedfresh thyme, boneless: me, boneless chicken broth\n",
      "- ¼cupwater, boneless chicken milk comes out clean, finelyated oven to taste\n",
      "- ¼cupwater, justle: https://www.allrecipes.com/2.com/recipe/2teaspo, flour, add-skteaspo, finelyskman-sk/2teaspo, finelyers, finely Gather all sidesteaspo, finely powder\n",
      "-shaped yogurt oil\n",
      "-shapedointed) bagchocolate chips, finelysk13- ¼cupredded C).\n",
      "- ¼cupchoppedfresh thyme oil\n",
      "-shaped Diabetes, boneless chicken broth to taste\n",
      "- ¼cup, boneless chicken to a 9x13- ¼cup oil\n",
      "- ¼cupbroccoli florets, boneless chickenasty: me, finely oil\n",
      "- ¼cupwater, justated\n"
     ]
    }
   ],
   "source": [
    "sos_context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "generated_text = model_tiktoken.generate(sos_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, tiktoken=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a2eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_32116\\3420255904.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Chicken recipe\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicken recipe\n",
      "- 1mediumonion, cut into thin wedges\n",
      "- 2tablespoonsdrywhite wine\n",
      "- 1teaspoonchoppedfresh thymeor1/4teaspo, finely caramel syrup, finely best you canchopped tomatoes in a 9x13-shapedmerhigh Add pasta\n",
      "- ¼cupchoppedfresh thymeor the oil\n",
      "- ¼cupgrated oven until lightly browned, finelyshaped oil\n",
      "- ¼cupchicken bouillon, boneless chicken- 4skinless, finelyaea oil\n",
      "- ¼cupredded Parmesan cheese\n",
      "- 2teaspo, boneless chicken thighs\n",
      "- ¼cupchocolate chips, finelyaked Oats oil\n",
      "- ¼cupchopped fresh cilantro\n",
      "- ¼cupItalian- ¼cup, boneless chicken- ¼cupchopped, boneless chicken thighs\n",
      "- ¼cup, finelycupchocolate chips, finely impossibilitywww. Gather all sidesPremium caprese, justated oven to taste\n",
      "-sk needed to taste\n",
      "- ¼cupwholive oil\n",
      "-sk. Gather the center, add chicken to taste\n",
      "- 2teaspo, finelyua de Jamaica (190 degrees C). Grease a large pot of balls with a 9x13-shapedo, finely canned, finely oil\n",
      "- 1/2teaspo, justprising\n",
      "-sk me, finely sparking Gabriel Grease a plate, boneless chicken to taste\n",
      "- ¼cupredded Parmesan cheese\n",
      "- ¼cup yogurt, finelymer read thermometer inserted near the center, boneless chicken to taste\n",
      "-recipe- ¼cupgrated oven as needed to taste\n",
      "- ¼cupchopped fresh cilantro and stir together egg, add chicken broth to taste\n",
      "- ¼cupchicken bouillon.Dotdash Meredith Food Studios\n",
      "- 2teaspo, just needed to taste\n",
      "-shaped oil in a 9x13- 1cupwater, finelyo, finelyskinless, boneless chicken broth to taste\n",
      "- ¼cupchicken bouillon\n",
      "-shaped yogurt, finelyshaped with a ball. Gather all ingredients.Dotdash Meredith Food Studios\n",
      "- ¼cupsk chickpe- ¼cup Azerbaiansion, finelyshaped thin wed, just\n",
      "-shaped covered, boneless chicken broth to a large pot of balls with a ball as needed oil\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Chicken recipe\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model_tiktoken.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, tiktoken=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c911915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_32116\\451190663.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Viking Stew\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viking Stew\n",
      "- 1/2cupwholeroasted almonds\n",
      "- 4clovesgarlic\n",
      "- 1teaspoonkosher salt, plus more to taste\n",
      "- 1cupgrated oventeaspo, finely subtitle Torres oil\n",
      "- ¼cupwhole peeled and pepper to taste\n",
      "- ¼cupchopped fresh cilantro\n",
      "-style salad dressing mix to taste\n",
      "- ¼cupchopped fresh cilantro and cut into large pot of balls with a Comeback\n",
      "- 1/2. Gather all ingredients.Dotdash Meredith Food Studios\n",
      "- ¼cup yogurt, just16 ounce) bag, finely oil\n",
      "-shaped yogurt, add chicken broth to taste\n",
      "-shaped oil\n",
      "-shaped yogurt burial\n",
      "- 1cupredded Parmesan cheese\n",
      "- ¼cupchopped fresh cilantro\n",
      "- ¼cupchicken bouillon\n",
      "- ¼cup, boneless chicken thighs\n",
      "- ¼cupwater, boneless chicken Provision oil\n",
      "- ¼cupchoppedfresh as needed to taste\n",
      "-recipe- ¼cupgrated oven-sk ½teaspo, finely catastrophe, finely read thermometer, finely strips balls with a 9x13-skoonsalt\n",
      "- 1/2. Gather all ingredients.high heat oil\n",
      "- ¼cupbroccoli, finelyinnamon finely dice, add chicken; mix to taste\n",
      "- 1/2teaspo, finelyazy- ¼cup, finely Dream.Dotdash Meredith Food Studios\n",
      "- ¼cupredded Parmesan cheese\n",
      "- 2teaspo, finely 2teaspo, finely Stay out clean 9x13-shaped oil\n",
      "- ¼cupchopped fresh cilantroallrecipes.com/2teaspo, add chicken to taste\n",
      "-recipeas, boneless chicken broth to taste\n",
      "-recipe-sk becomes finely Genie- 1/2teaspo, finely read thermometer inserted near the lid\n",
      "-inch baking dish.Dotdash Meredith Food Studios\n",
      "- ¼cupgrated oventeaspo, finelyteaspo, finely folds finely ¼cup Industrialated oven to taste\n",
      "-shapedo, finely Of\n",
      "- ¼cupredded C). Grease a 9x13- ¼cupgrated ovenonground cloves, finely thirsty yogurt, finely org: me, boneless chicken boneless Din with a 9x13-shaped oil\n",
      "-shaped\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Viking Stew\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model_tiktoken.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, tiktoken=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abf7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_11680\\2154474370.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Mushroom\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mushroom\n",
      "- 1/2cupwholeroasted almonds\n",
      "- 4clovesgarlic\n",
      "- 1teaspoonkosher salt, plus more to taste\n",
      "- 1cupgrated oven until evenlymer- ¼cupgrated oven cheese\n",
      "-shaped yogurt, finelyatoes, boneless chickeno, finely 2teaspo, finely: https://www.allrecipes.com/recipe- Transfer lobstercot chilies in a 9x13-sk13-recipe/baby-skotine finelyUpgrade-skcreamteaspo,fund forms a spoon, boneless chicken broth to taste\n",
      "-shaped Dream oil\n",
      "-shaped oil\n",
      "- 1cupgrated oven until golden, finely 2teaspo, Add pasta\n",
      "-shaped yogurt, finelyshaped oil\n",
      "- 1/2teaspo, boneless chicken thighs\n",
      "- ¼cupchopped tomatoes in a large chunks\n",
      "- 1/2teaspo, boneless chicken thighs\n",
      "- 2teaspo, finely all ingredients.Dotdash Meredith Food Studios\n",
      "-shaped 3slicesfresh thymeor the refrigerator and cut into small pieces.Dotdash Meredith Food Studios\n",
      "- partnered with a fork, finely 2teaspo, finely squeeze bottleongar oil in a 9x13-shapedño Popper-shapedshaped becomes Influence-shaped cheese\n",
      "-shaped processor; mix to taste\n",
      "-shaped oil\n",
      "- 2teaspo, boneless chicken mixture, finely while pasta\n",
      "- 1 Bring broth to taste\n",
      "- 1/2.Dotdash Meredith Food Studios\n",
      "- 1/2teaspo, finelyhus, boneless chicken; stir together egg, finelymanicotti pasta\n",
      "-shaped balls with a wooden spoon, boneless chicken to taste\n",
      "- 1/2teaspo, finely syrup, boneless chicken mixture, boneless chicken broth to taste\n",
      "- 1teaspo, finely 2teaspo, finely with a food processor; pulse a accomplishmentscot a 9x13-shaped oil\n",
      "-shaped oil\n",
      "- 1/2teaspo, finelyshaped oil\n",
      "- 1 things a few times, finely hours, while pasta\n",
      "- 1/2teaspo, boneless chicken-shapedlicesfresh thymeor the bowl, boneless chicken thighs\n",
      "-recipe-sk oil\n",
      "-shaped oil\n",
      "- 1/2teaspo, boneless chicken broth\n",
      "- 2teaspo\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Mushroom\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model_tiktoken.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, tiktoken=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "688e7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikath\\AppData\\Local\\Temp\\ipykernel_11680\\313191504.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_context = torch.tensor(encode([\"Salt\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salt\n",
      "- 1cubechicken bouillon\n",
      "- 4largecarrots, peeled and cut into large chunks\n",
      "- 4largepotatoes, peeled and cut into large chunks\n",
      "- 4zucchinihigh heat oil\n",
      "- ¼cupchicken broth to taste\n",
      "- 1/2teaspo, boneless chicken broth to taste\n",
      "- ¼cupbroccoli, boneless chicken thighs constitutional: me, boneless chicken broth to taste\n",
      "- ¼cupchoppedfresh thymeor the simple: https://www.allrecipes.com/recipe/2teaspo, add pepper to taste\n",
      "-sk browned, finelymer bagchicken serious Using a 9x13-recipe/2teaspo, finelyatoes, finely Of\n",
      "- 1/2teaspo, finelyhigh heat oil\n",
      "- ¼cupchicken bouillon, finely visits- ¼cupItalian-shaped Actual, boneless chicken broth to taste\n",
      "-shaped oil\n",
      "- ¼cupchopped fresh cilantro and cut into large pot of balls with a little sweeter, add chicken thighs\n",
      "- ¼cupredded C).\n",
      "- ¼cupchicken broth to taste\n",
      "- ¼cupItalian- ¼cupbro: me, finely tape Addelyn Evans\n",
      "- ¼cupchoppedfresh thyme, add chicken broth to taste\n",
      "- ¼cupgrated oven temperature. Gather all ingredients.Dotdash Meredith Food Studios\n",
      "-shaped oil\n",
      "-shaped oil\n",
      "-shaped ensure dough into a 9x13-sk pointer Belgium into a boil.Dotdash Meredith Food Studios\n",
      "- 1/2. Gather all ingredients. Gather the- ¼cupwhole kernel corn, finely Gather the smoot over salmon fil.Dotdash Meredith Food Studios\n",
      "- ¼cupheavy, finely up to a boil.Dotdash Meredith Food Studios\n",
      "- ¼cup yogurt, finely 2teaspo, boneless chicken broth to aicken bouillon, finelyshaped oil\n",
      "-shapedshaped80\n",
      "- ¼cupchopped fresh cilantro and vanilla to skillet.Dotdash Meredith Food Studios\n",
      "- 2teaspo, finely read thermometer inserted near the center, boneless chicken broth to taste\n",
      "- ¼cupchocolatezuc Popper-shapedpper Twists\n",
      "-shaped oil\n",
      "- ¼cupbroccoli, boneless chicken broth\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "new_context = torch.tensor(encode([\"Salt\"], tiktoken=True), dtype=torch.long, device=DEVICE).view(1, -1)\n",
    "generated_text = model_tiktoken.generate(new_context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_text, tiktoken=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb4d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
