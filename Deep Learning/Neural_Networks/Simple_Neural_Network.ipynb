{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network from scratch\n",
    "\n",
    "Building a neural network with one hidden layer, using forward propagation and backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vuhF9_IXMES6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.backend import epsilon\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Layer base class.\n",
    "        \"\"\"\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        \"\"\"Computes the output Y of a layer for a given input X.\n",
    "\n",
    "        Args:\n",
    "            input: input to the layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"Computes dE/dX for a given dE/dY (and update parameters if any).\n",
    "\n",
    "        Args:\n",
    "            output_error: output error.\n",
    "            learning_rate: learning Rate.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"Fully Connected Layer.\n",
    "\n",
    "        Args:\n",
    "            input_size: number of input neurons.\n",
    "            output_size: number of output neurons.\n",
    "        \"\"\"\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        \"\"\"Function to apply forward propogation.\n",
    "\n",
    "        Args:\n",
    "            input_data: input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            self.output: output for given input.\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"Function to apply backward propogation.\n",
    "\n",
    "        Args:\n",
    "            output_error: output error.\n",
    "            learning_rate: learning Rate.\n",
    "\n",
    "        Returns:\n",
    "            input_error: dE/dX for a given output_error=dE/dY\n",
    "        \"\"\"\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        \"\"\"Activation Layer.\n",
    "\n",
    "        Args:\n",
    "            activation: activation function.\n",
    "            activation_prime: derivative of activation function.\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        \"\"\"Function to apply forward propogation.\n",
    "\n",
    "        Args:\n",
    "            input_data: input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            self.output: activated input.\n",
    "        \"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"Function to apply backward propogation.\n",
    "\n",
    "        Args:\n",
    "            output_error: output error.\n",
    "            learning_rate: learning Rate.\n",
    "\n",
    "        Returns:\n",
    "            input_error: dE/dX for a given output_error=dE/dY\n",
    "        \"\"\"\n",
    "        return self.activation_prime(self.input) * output_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function - Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Implementing the sigmoid function for x.\n",
    "    sig(x) = 1/(1+e^-x)\n",
    "\n",
    "    Args:\n",
    "        x: input for which sigmoid function needs to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"Derivative of the sigmoid function.\n",
    "    sig'(x) = sig(x) * (1-sig(x))\n",
    "\n",
    "    Args:\n",
    "        x: input for which derivative of signmoid function needs to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        the derivative of sigmoid function.\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function - Cross Entropy\n",
    "\n",
    "<img src=\"http://androidkt.com/wp-content/uploads/2021/05/Selection_099-1024x200.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_true, y_pred):\n",
    "    \"\"\"Implementing cross entropy loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: true value of input.\n",
    "        y_pred: predicted value of input.\n",
    "\n",
    "    Returns:\n",
    "        the cross entropy loss.\n",
    "    \"\"\"\n",
    "    if y_true == 1:\n",
    "        return -np.log(y_pred)\n",
    "\n",
    "    else:\n",
    "        return -np.log(1 - y_pred)\n",
    "\n",
    "\n",
    "def bce_prime(y_true, y_pred):\n",
    "    \"\"\"Implementing derivative of cross entropy loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: true value of input.\n",
    "        y_pred: predicted value of input.\n",
    "\n",
    "    Returns:\n",
    "        the cross entropy loss.\n",
    "    \"\"\"\n",
    "    if y_true == 1:\n",
    "        return -1 / y_pred\n",
    "\n",
    "    else:\n",
    "        return 1 / (1 - y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all the functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Neural Network.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"Add layer to network.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        \"\"\"Set loss to use.\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Predict output for given input.\n",
    "        \"\"\"\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        \"\"\"Train the network.\n",
    "        \"\"\"\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i + 1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "[Dataset on kaggle](https://www.kaggle.com/omnamahshivai/surgical-dataset-binary-classification)\n",
    "\n",
    "Attribute Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "original_df = pd.read_csv('Surgical-deepnet.csv')\n",
    "\n",
    "x = original_df.iloc[:, :-1].values\n",
    "y = original_df['complication'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  (10244, 1, 24)\n",
      "Testing samples:  (4391, 24)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=1)\n",
    "\n",
    "print('Training samples: ', x_train.shape)\n",
    "print('Testing samples: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "\n",
    "# input_shape=(1, 24)       ;   output_shape=(1, 100)\n",
    "net.add(FCLayer(24, 100))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(FCLayer(100, 50))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# input_shape=(1, 50)       ;   output_shape=(1, 1)\n",
    "net.add(FCLayer(50, 1))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/50   error=0.555069\n",
      "epoch 2/50   error=0.547742\n",
      "epoch 3/50   error=0.561009\n",
      "epoch 4/50   error=0.559113\n",
      "epoch 5/50   error=0.551968\n",
      "epoch 6/50   error=0.555554\n",
      "epoch 7/50   error=0.553901\n",
      "epoch 8/50   error=0.548685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train on all samples\u001b[39;00m\n\u001b[0;32m      2\u001b[0m net\u001b[38;5;241m.\u001b[39muse(bce, bce_prime)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[1;34m(self, x_train, y_train, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     57\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_prime(y_train[j], output)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 59\u001b[0m         error \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# calculate average error on all samples\u001b[39;00m\n\u001b[0;32m     62\u001b[0m err \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m samples\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mFCLayer.backward_propagation\u001b[1;34m(self, output_error, learning_rate)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"Function to apply backward propogation.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    input_error: dE/dX for a given output_error=dE/dY\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m input_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(output_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 37\u001b[0m weights_error \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# dBias = output_error\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# update parameters\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m weights_error\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train on all samples\n",
    "net.use(bce, bce_prime)\n",
    "\n",
    "net.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[0.14884123]]), array([[0.27856554]]), array([[0.04212324]])]\n",
      "true values : \n",
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# test on 3 samples\n",
    "out = net.predict(x_test[:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>True Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.306132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>0.188757</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4387</th>\n",
       "      <td>0.360816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>0.042123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4389</th>\n",
       "      <td>0.125844</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4390</th>\n",
       "      <td>0.083374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4391 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predictions  True Values\n",
       "0        0.148841            1\n",
       "1        0.278566            1\n",
       "2        0.042123            0\n",
       "3        0.065332            0\n",
       "4        0.306132            1\n",
       "...           ...          ...\n",
       "4386     0.188757            0\n",
       "4387     0.360816            1\n",
       "4388     0.042123            0\n",
       "4389     0.125844            0\n",
       "4390     0.083374            0\n",
       "\n",
       "[4391 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [i[0][0] for i in net.predict(x_test)]\n",
    "pd.DataFrame.from_dict({\n",
    "    'Predictions': preds,\n",
    "    'True Values': y_test\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Complexity\n",
    "\n",
    "Time complexity is the amount of time taken by an algorithm to run, as a function of the length of the input. It measures the time taken to execute each statement of code in an algorithm.\n",
    "\n",
    "Our Neural Network consists of 2 input nodes going into a hidden layer with 3 nodes which in turn goes to the output layer with 1 node. The weighted sum of each layer is going through a sigmoid activation function.\n",
    "\n",
    "<img src=\"NN.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(time):\n",
    "    return datetime.timedelta(\n",
    "        hours=time.hour, minutes=time.minute, seconds=time.second, microseconds=time.microsecond\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for initial_time, final_time, epochs in zip(all_initial_times, all_final_times, all_epochs):\n",
    "    initial_timedelta = convert_time(initial_time)\n",
    "    final_timedelta = convert_time(final_time)\n",
    "\n",
    "    # total time taken by program\n",
    "    print('Total time taken for %7d epochs: %18s' %\n",
    "          (epochs, final_timedelta - initial_timedelta))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NN_OR_Gate_via_Backprop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
